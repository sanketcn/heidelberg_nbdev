{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda to Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import os\n",
    "import csv\n",
    "\n",
    "STREAM_NAME=os.environ['STREAM_NAME']\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    print(\"Incoming Event: \", event);\n",
    "    print(\"bucket \", context)\n",
    "    \n",
    "    # csvfile = s3.get_object(Bucket=bucket, Key=file_key)\n",
    "    #csvcontent = csvfile['Body'].read().split(b'\\n')\n",
    "    #csv_data = csv.DictReader(csvcontent)\n",
    "    \n",
    "    records = event['Records']\n",
    "    \n",
    "    key=records['s3']['object']['key']\n",
    "    bucket=records['s3']['bucket']['name']\n",
    "    \n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_object = s3_resource.Object(bucket, key)\n",
    "\n",
    "    data = s3_object.get()['Body'].read().decode('utf-8').splitlines()\n",
    "\n",
    "    lines = csv.reader(data)\n",
    "    headers = next(lines)\n",
    "    #print('headers: %s' %(headers))\n",
    "    payloads = []\n",
    "    # refernece:- https://stackoverflow.com/questions/56849240/how-to-read-csv-file-from-s3-bucket-in-aws-lambda\n",
    "    for line in lines:\n",
    "        #print complete line\n",
    "        #print(line)\n",
    "        #print index wise\n",
    "        #print(line[0], line[1])\n",
    "        j=0\n",
    "        d={}\n",
    "        for i in headers:\n",
    "            if(i!=\"target\"):\n",
    "                d[i]=line[j]\n",
    "            j=j+1\n",
    "\n",
    "        payloads.append(d)\n",
    "    \n",
    "    for payload in payloads:\n",
    "        put_to_stream(payload,time.time(),STREAM_NAME)\n",
    "        \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Hello from Lambda!')\n",
    "    }\n",
    "    \n",
    "    \n",
    "def put_to_stream(payload,timestamp,STREAM_NAME):\n",
    "    ret_status = True\n",
    "    data = json.dumps(payload)\n",
    "    print(f'Sending a new payload: ', payload)\n",
    "    response = kinesis_client.put_record(StreamName = STREAM_NAME ,\n",
    "                                             Data = data,\n",
    "                                             PartitionKey = 'shard1')\n",
    "    \n",
    "    if (response['ResponseMetadata']['HTTPStatusCode'] != 200):\n",
    "        print(\"ERROR: Kinesis put_record failed: \\n{}\".format(json.dumps(response)))\n",
    "        ret_status = False\n",
    "        \n",
    "    return ret_status\n",
    "\n",
    "\n",
    "\n",
    "def get_cloudwatch_logs_url(start_time, end_time,predict_lambda_name):\n",
    "    log_group_name = '/aws/lambda/' + predict_lambda_name \n",
    "    # get the latest log stream for our Lambda that makes fraud predictions\n",
    "    cw_client = boto3.client('logs')\n",
    "    last_cw_evt = 0\n",
    "    while last_cw_evt < int(start_test_time * 1000):\n",
    "        streams = cw_client.describe_log_streams(logGroupName=log_group_name,\n",
    "                                                 orderBy='LastEventTime',\n",
    "                                                 descending=True)['logStreams']\n",
    "        last_cw_evt = streams[0]['lastIngestionTime'] #'lastEventTimestamp']\n",
    "        latest_stream = str(streams[0]['logStreamName']).replace('/', '$252F').replace('[$LATEST]', '$255B$2524LATEST$255D')\n",
    "        if last_cw_evt < int(start_test_time * 1000):\n",
    "            print('waiting for updated log stream...')\n",
    "            time.sleep(10)\n",
    "\n",
    "    # produce a valid URL to get to that log stream\n",
    "    region = boto3.session.Session().region_name\n",
    "    log_group_escaped = log_group_name.replace('/', '$252F')\n",
    "    cw_url = f'https://console.aws.amazon.com/cloudwatch/home?region={region}#logsV2:log-groups/log-group/{log_group_escaped}'\n",
    "    time_filter = f'$26start$3D{int(start_test_time * 1000) - 10000}$26end$3D{int(end_test_time * 1000) + 40000}'\n",
    "    full_cw_url = f'{cw_url}/log-events/{latest_stream}$3FfilterPattern$3DPrediction+{time_filter}'\n",
    "    print('Updated log stream is ready.')\n",
    "    return full_cw_url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lamnda to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import base64\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Read environment variables\n",
    "ENDPOINT_NAME = os.environ['ENDPOINT_NAME']\n",
    "FEATURE_GROUP = os.environ['FEATURE_GROUP_NAME']\n",
    "FRAUD_THRESHOLD = os.environ['FRAUD_THRESHOLD']\n",
    "LOG_LEVEL = os.environ['LOG_LEVEL']\n",
    "\n",
    "logger = logging.getLogger()\n",
    "if logging._checkLevel(LOG_LEVEL):\n",
    "    logger.setLevel(LOG_LEVEL)\n",
    "else:\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "logging.info(f'Setting Logger Level to {logging.getLevelName(logger.level)}')\n",
    "\n",
    "import boto3\n",
    "\n",
    "print(f'boto3 version: {boto3.__version__}')\n",
    "# Create session via Boto3\n",
    "session = boto3.session.Session()\n",
    "\n",
    "try:\n",
    "    featurestore_runtime = boto3.Session().client(service_name='sagemaker-featurestore-runtime')\n",
    "except:\n",
    "    logging.error('Failed to instantiate featurestore-runtime client with install.sh script!')\n",
    "\n",
    "# Allocate SageMaker runtime\n",
    "sagemaker_runtime = boto3.client('runtime.sagemaker')\n",
    "\n",
    "logging.info(f'Lambda will call SageMaker ENDPOINT name: {ENDPOINT_NAME}')\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\" This handler is triggered by incoming Kinesis events,\n",
    "    which contain a payload encapsulating the transaction data.\n",
    "    The Lambda will then lookup corresponding records in the\n",
    "    aggregate feature groups, assemble a payload for inference,\n",
    "    and call the inference endpoint to generate a prediction.\n",
    "    \"\"\"\n",
    "    logging.debug('Received event: {}'.format(json.dumps(event, indent=2)))\n",
    "\n",
    "    records = event['Records']\n",
    "    logging.debug('Event contains {} records'.format(len(records)))\n",
    "    \n",
    "    ret_records = []\n",
    "    for rec in records:\n",
    "        # Each record has separate eventID, etc.\n",
    "        event_id = rec['eventID']\n",
    "        event_source_arn = rec['eventSourceARN']\n",
    "        logging.debug(f'eventID: {event_id}, eventSourceARN: {event_source_arn}')\n",
    "\n",
    "        kinesis = rec['kinesis']\n",
    "        event_payload = decode_payload(kinesis['data'])\n",
    "\n",
    "\n",
    "        feature_string = assemble_features(event_payload)\n",
    "        prediction = invoke_endpoint(feature_string)\n",
    "        \n",
    "        if prediction is not None:\n",
    "            sequence_num = kinesis['sequenceNumber']\n",
    "            ret_records.append({'eventId': event_id,\n",
    "                            'sequenceNumber': sequence_num,\n",
    "                            'prediction': prediction,\n",
    "                            'statusCode': 200})\n",
    "\n",
    "    return ret_records\n",
    "                       \n",
    "def assemble_features( event_payload ):\n",
    "    inference_features = []\n",
    "    features = []\n",
    "    for feature in features:\n",
    "        inference_features.append(str(event_payload[feature]))\n",
    "\n",
    "    logging.debug(f'Inference features: {inference_features}')\n",
    "\n",
    "    # assemble features into CSV-format string\n",
    "    feature_string = ','.join(inference_features)\n",
    "\n",
    "    return feature_string\n",
    "\n",
    "\n",
    "def invoke_endpoint(request_body):\n",
    "    logging.debug('Passing Request Body (CSV-format): {}'.format(request_body))\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=ENDPOINT_NAME,\n",
    "        ContentType='text/csv',\n",
    "        Body=request_body)        \n",
    "    logging.info('Inference Response: {}'.format(response))\n",
    "\n",
    "    probability = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    return probability\n",
    "\n",
    "\n",
    "def decode_payload(event_data):\n",
    "    agg_data_bytes = base64.b64decode(event_data)\n",
    "    decoded_data = agg_data_bytes.decode(encoding=\"utf-8\") \n",
    "    event_payload = json.loads(decoded_data) \n",
    "    logging.info(f'Decoded data from kinesis record: {event_payload}')\n",
    "    return event_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lamnda to FeatureStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import base64\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import boto3\n",
    "\n",
    "print(f'boto3 version: {boto3.__version__}')\n",
    "\n",
    "try:\n",
    "    sm = boto3.Session().client(service_name='sagemaker')\n",
    "    sm_fs = boto3.Session().client(service_name='sagemaker-featurestore-runtime')\n",
    "except:\n",
    "    print(f'Failed while connecting to SageMaker Feature Store')\n",
    "    print(f'Unexpected error: {sys.exc_info()[0]}')\n",
    "\n",
    "\n",
    "# Read Environment Vars\n",
    "FEATURE_GROUP = os.environ['FEATURE_GROUP_NAME']\n",
    "\n",
    "features = ['kst_brutto', 'sm', 'tm', 'cl', 'so3', 'k2o', 'na2o', 'south_kiln_feed_01om886__tph__avg', 'south_kiln_feed_01om886__tph__max', 'north_kiln_feed_01om885__tph__avg', 'north_kiln_feed_01om885__tph__max', 'north_fan_speed_01oa943__rpm__avg', 'north_fan_speed_01oa943__rpm__max', 'south_fan_speed_02oa943__rpm__avg', 'south_fan_speed_02oa943__rpm__max', 'lignite_main_burner_03sk820__tph__avg', 'lignite_main_burner_03sk820__tph__max', 'bpg_main_burner_03bf810__tph__avg', 'bpg_main_burner_03bf810__tph__max', 'lignite_calciner_02sk820__tph__avg', 'lignite_calciner_02sk820__tph__max', 'bpg_calciner_02bf810__tph__avg', 'bpg_calciner_02bf810__tph__max', 'kbs_calciner_00kb950__tph__avg', 'kbs_calciner_00kb950__tph__max', 'total_energy_to_main_burner__gj_h__avg_0', 'total_energy_to_main_burner__gj_h__max_0', 'total_energy_to_main_burner__gj_h__avg_1', 'total_energy_to_main_burner__gj_h__max_1', 'total_energy_to_main_burner__gj_h__avg_2', 'total_energy_to_main_burner__gj_h__max_2', 'total_energy_to_main_burner__gj_h__avg_3', 'total_energy_to_main_burner__gj_h__max_3', 'total_energy_to_calciner__gj_h__avg_0', 'total_energy_to_calciner__gj_h__max_0', 'total_energy_to_calciner__gj_h__avg_1', 'total_energy_to_calciner__gj_h__max_1', 'total_energy_to_calciner__gj_h__avg_2', 'total_energy_to_calciner__gj_h__max_2', 'total_energy_to_calciner__gj_h__avg_3', 'total_energy_to_calciner__gj_h__max_3', 'tf__traditional_fuel__avg', 'tf__traditional_fuel__max', 'af__alternative_fuel__avg', 'af__alternative_fuel__max', 'shc__gj__t_kiln_feed__avg', 'shc__gj__t_kiln_feed__max', 'nox_concentration_in_kiln_inlet_00oa983__ppm__avg_0', 'nox_concentration_in_kiln_inlet_00oa983__ppm__max_0', 'nox_concentration_in_kiln_inlet_00oa983__ppm__avg_1', 'nox_concentration_in_kiln_inlet_00oa983__ppm__max_1', 'nox_concentration_in_kiln_inlet_00oa983__ppm__avg_2', 'nox_concentration_in_kiln_inlet_00oa983__ppm__max_2', 'nox_concentration_in_kiln_inlet_00oa983__ppm__avg_3', 'nox_concentration_in_kiln_inlet_00oa983__ppm__max_3', 'co_concentration_in_kiln_inlet_00oa981__ppm__avg', 'co_concentration_in_kiln_inlet_00oa981__ppm__max', 'o2_concentration_in_kiln_inlet_00oa982_____avg', 'o2_concentration_in_kiln_inlet_00oa982_____max', 'bottom_cyclone_gas_temperature_00oa800___c___avg', 'bottom_cyclone_gas_temperature_00oa800___c___max', 'secondary_air_temperature_01kk827___c___avg_0', 'secondary_air_temperature_01kk827___c___max_0', 'secondary_air_temperature_01kk827___c___avg_1', 'secondary_air_temperature_01kk827___c___max_1', 'secondary_air_temperature_01kk827___c___avg_2', 'secondary_air_temperature_01kk827___c___max_2', 'secondary_air_temperature_01kk827___c___avg_3', 'secondary_air_temperature_01kk827___c___max_3', 'tertiary_air_temperature_01kb801___c___avg_0', 'tertiary_air_temperature_01kb801___c___max_0', 'tertiary_air_temperature_01kb801___c___avg_1', 'tertiary_air_temperature_01kb801___c___max_1', 'tertiary_air_temperature_01kb801___c___avg_2', 'tertiary_air_temperature_01kb801___c___max_2', 'tertiary_air_temperature_01kb801___c___avg_3', 'tertiary_air_temperature_01kb801___c___max_3', 'kiln_amps_01do812__amps__avg_0', 'kiln_amps_01do812__amps__max_0', 'kiln_amps_01do812__amps__avg_1', 'kiln_amps_01do812__amps__max_1', 'kiln_amps_01do812__amps__avg_2', 'kiln_amps_01do812__amps__max_2', 'kiln_amps_01do812__amps__avg_3', 'kiln_amps_01do812__amps__max_3', 'co_bottom_cyclone_north_00oa941__amps__avg', 'co_bottom_cyclone_north_00oa941__amps__max', 'o2_bottom_cyclone_north_00oa942_____avg_0', 'o2_bottom_cyclone_north_00oa942_____max_0', 'o2_bottom_cyclone_north_00oa942_____avg_1', 'o2_bottom_cyclone_north_00oa942_____max_1', 'o2_bottom_cyclone_north_00oa942_____avg_2', 'o2_bottom_cyclone_north_00oa942_____max_2', 'o2_bottom_cyclone_north_00oa942_____avg_3', 'o2_bottom_cyclone_north_00oa942_____max_3', 'kiln_speed_01do811__rpm__avg', 'kiln_speed_01do811__rpm__max', 'primary_air_amount_01of850__nm3_h___avg', 'primary_air_amount_01of850__nm3_h___max', 'axial_air_pressure_01of811__mbar__avg', 'axial_air_pressure_01of811__mbar__max', 'radial_air_pressure_01of812__mbar__avg', 'radial_air_pressure_01of812__mbar__max', 'central_air_pressure_01of813__mbar__avg', 'central_air_pressure_01of813__mbar__max', 'total_feed_avg', 'total_feed_max', 'total_energy_avg', 'total_energy_max', 'average_fan_speed_avg', 'average_fan_speed_max', 'average_air_temperature_avg', 'average_air_temperature_max', 'kiln_feed_rate_avg', 'kiln_feed_rate_max', 'klinker___nk_c3s_lag_shift', 'klinker___nk_mgo_lag_shift', 'klinker___nk_fe2o3_lag_shift', 'klinker___nk_al2o3_lag_shift', 'klinker___nk_na2o_lag_shift', 'klinker___nk_k2o_lag_shift', 'klinker___nk_cao_lag_shift', 'klinker___nk_sio2_lag_shift', 'heat_of_formation__kcal_kg_clinker__lag_shift', 'klinker___nk_so3_lag_shift', 'klinker___nk_sm_lag_shift', 'klinker___nk_am_lag_shift', 'klinker___nk_lsf_lag_shift', 'id']\n",
    "\n",
    "def update_agg(fg_name, data):\n",
    "    record = []\n",
    "    for feature in features:\n",
    "        temp = {'FeatureName': feature, 'ValueAsString' : str(data[feature])}\n",
    "        record.append(temp)\n",
    "    sm_fs.put_record(FeatureGroupName=fg_name, Record=record)\n",
    "    return\n",
    "        \n",
    "def lambda_handler(event, context):\n",
    "    inv_id = event['invocationId']\n",
    "    app_arn = event['applicationArn']\n",
    "    records = event['records']\n",
    "    print(f'Received {len(records)} records, invocation id: {inv_id}, app arn: {app_arn}')\n",
    "    \n",
    "    ret_records = []\n",
    "    for rec in records:\n",
    "        raw_data = rec['data']\n",
    "        data_str = base64.b64decode(raw_data) \n",
    "        data = json.loads(data_str)\n",
    "        print(f' updating features for id: {data}')\n",
    "        update_agg(FEATURE_GROUP, data)\n",
    "        \n",
    "        # Flag each record as being \"Ok\", so that Kinesis won't try to re-send \n",
    "        ret_records.append({'recordId': rec['recordId'],\n",
    "                            'result': 'Ok'})\n",
    "    return {'records': ret_records}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
